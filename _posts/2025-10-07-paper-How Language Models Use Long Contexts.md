---
title: Lost in the Middle - How Language Models Use Long Contexts 
date: 2025-10-07 11:00:00 +0800
categories: [Paper, LLM]
tags: [few-shot]
render_with_liquid: false
---

这篇论文 **《Lost in the Middle: How Language Models Use Long Contexts》**（2023年由斯坦福大学等团队发表）揭示了大语言模型（LLM）在处理长文本时的一个“扎心”事实：**模型并不是对整段输入一视同仁的。**

该研究对我们现在如何使用 RAG（检索增强生成）和构建长文本提示词（Prompt）产生了深远影响。以下是详细解读：

---

### 1. 核心发现：U型性能曲线

论文通过一系列实验（如多文档问答和键值对检索）发现，模型对信息的利用能力呈现明显的 **“U型曲线”**：

* **首尾效应（Primacy & Recency Bias）**：模型最容易记住并利用出现在文本**开头**和**结尾**的信息。
* **中间迷失（Lost in the Middle）**：当关键信息出现在长文本的**中间位置**时，模型的准确率会大幅下降，甚至有时表现得像根本没看过这些信息一样。

---

### 2. 为什么会“迷失”？

作者分析了几个可能的原因：

* **模型架构（Transformer 限制）**：由于自注意力机制（Self-Attention）的复杂度和训练时的截断，模型可能天生更关注序列的两端。
* **训练数据的偏差**：在预训练和指令微调阶段，重要的指令或关键结论往往出现在文档的开头或结尾，导致模型形成了这种偏好。
* **上下文长度的压力**：即使模型标称支持 128k 甚至更长的上下文，随着输入增加，信噪比降低，模型在处理“中间”的海量信息时会出现推理能力的疲劳。

---

### 3. 对实际应用的重大启示

这篇论文不仅是学术研究，更是给开发者们的“避坑指南”：

* **RAG 系统的重排（Re-ranking）**：
* **痛点**：如果你从向量数据库检索出 20 个相关片段并直接塞给模型，最相关的那个偏偏排在第 10 位，模型很可能会忽略它。
* **对策**：必须在检索后进行 **Re-ranking**。将最关键的信息手动移动到提示词的最前面或最后面。


* **提示词工程（Prompt Engineering）**：
* **对策**：在写长提示词时，把核心任务说明（Instructions）和最重要的参考资料放在开头或结尾，而不要堆在中间。


* **不要迷信“超长上下文”**：
* 论文指出，即便是一些专门针对长文本优化的模型（如当时的 GPT-4-32k、Claude-v1.3），依然无法完全避免“中间迷失”的问题。**“能装下”不代表“能用好”。**



---

### 4. 实验细节

* **多文档问答**：给模型 20 个文档，其中只有一个包含答案。改变这个文档的位置，观察准确率。
* **键值检索**：构建一个类似字典的列表，要求模型根据 Key 找 Value。当 KV 对在列表中间时，性能最差。
* **模型覆盖**：测试了 GPT-3.5、GPT-4、Claude 等多种主流闭源模型，以及 Llama-2 等开源模型，发现该现象普遍存在。

---

### 总结

这篇论文打破了“上下文窗口越大越好”的幻想。它告诉我们：**在长文本时代，信息的排列顺序（Order）和信息的质量（Quality）同样重要。**
