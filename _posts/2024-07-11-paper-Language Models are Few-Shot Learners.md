---
title: Language Models are Few-Shot Learners 
date: 2024-07-11 11:00:00 +0800
categories: [Paper, LLM]
tags: [few-shot]
render_with_liquid: false
math: true
---

这篇论文由 OpenAI 团队于 2020 年发表，正式推出了 **GPT-3**。这篇论文则彻底改变了我们对“AI 到底该怎么用”的认知。

它最核心的贡献是证明了：**当模型足够大时，它不需要针对特定任务进行微调（Fine-tuning），只需给它几个例子（Few-shot），它就能学会新任务。**

---

### 1. 核心概念：情境学习 (In-Context Learning)

在 GPT-3 之前，主流做法是“预训练 + 微调”。而这篇论文提出，人类学习新任务往往只需要看一两个例子，AI 应该也行。

论文重点讨论了三种模式：

* **Zero-shot (零样本)**：只给指令，不给例子。（如：“将‘你好’翻译成英文：”）
* **One-shot (单样本)**：给指令 + 一个例子。
* **Few-shot (少量样本)**：给指令 + 几个例子（通常 10 到 100 个）。

**结论：** 随着参数量增加，模型在 Few-shot 模式下的表现呈现出爆发式增长，甚至在很多任务上达到了与微调模型持平的水平。

---

### 2. “大”就是一切 (Scaling Laws)

GPT-3 拥有 **1750 亿 (175B)** 参数，比当时的 SOTA 模型 GPT-2 大了 100 倍。

* **架构没变**：GPT-3 依然沿用了 GPT-2 的 Transformer Decoder 架构，唯一的改动是在注意力层使用了交替的密集型和稀疏型注意力（Sparse Attention）。
* **暴力美学**：论文证明了性能与**参数量、数据集大小、计算量**三者之间存在幂律关系。只要堆算力和数据，模型就会变聪明。

---

### 3. 令人惊叹的能力

GPT-3 展示了许多之前小模型无法企及的能力：

* **算术运算**：能做两位数甚至三位数的加减法（虽然不完美，但展现了某种程度的推理）。
* **常识推理**：在需要“理解”物理常识的问题上表现出色。
* **代码生成**：初具规模的代码补全能力。
* **实时纠错**：能够识别并修正拼写错误。

---

### 4. 论文的深远影响

* **范式的转移**：它让工业界意识到，**提示工程 (Prompt Engineering)** 正在取代繁琐的微调。我们不再需要为每个任务训练一个小模型，而是去“调教”一个全能的大模型。
* **算力竞赛的开端**：175B 的参数量在当时被认为是“离谱”的，但它证明了这种投入的回报是巨大的，从而开启了大模型算力竞赛。
* **局限性分析**：论文也诚实地指出，GPT-3 依然存在局限，如逻辑推理依然薄弱、会产生事实性错误（幻觉）、对长文本的理解受限等。

---

### 5. 关键技术指标

| 特性 | GPT-2 (2019) | GPT-3 (2020) |
| --- | --- | --- |
| **参数量** | 1.5 Billion | **175 Billion** |
| **层数** | 48 | 96 |
| **词表大小** | 50,257 | 50,257 |
| **最大上下文** | 1024 tokens | 2048 tokens |

---

### 总结

这篇论文的标题 *“Language Models are Few-Shot Learners”* 是一次宣言：语言模型不仅仅是用来预测下一个词的概率工具，它们正在进化成能够理解指令并执行任务的**通用推理引擎**。
