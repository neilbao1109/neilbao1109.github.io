---
title: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models 
date: 2025-03-08 10:10:00 +0800
categories: [Paper, Prompting]
tags: [cot]
render_with_liquid: false
---

这篇由 Google Brain 团队于 2022 年发表的论文 **《Chain-of-Thought Prompting Elicits Reasoning in Large Language Models》**（思维链提示激发大语言模型的推理能力）是 AI 发展史上的一个里程碑。

它彻底改变了我们与大模型（LLM）交互的方式，从简单的“问答”转向了“引导思考”。以下是对这篇论文的深度解读：

---

## 1. 核心痛点：大模型的“偏科”

在 CoT 出现之前，即便像 GPT-3 这样强大的模型，在处理**算术推理**（如应用题）、**常识推理**和**符号推理**时也经常翻车。

* **传统做法：** 给模型几个“输入-输出”的例子（Few-shot Prompting），然后直接问答案。
* **结果：** 模型虽然懂语法，但由于缺乏逻辑推理的中间步骤，往往直接给出一个错误的数字或结论。

---

## 2. 什么是思维链（Chain of Thought）？

论文提出了一个极其简单但强大的想法：**在给出答案之前，强制模型生成一系列中间逻辑推理步骤。**

### 对比示例：

* **标准提示（Standard Prompting）：**
* 问：小明有5个苹果，吃了2个，又买了3个，现在有几个？
* 答：8个。（模型可能直接盲猜数字）


* **思维链提示（CoT Prompting）：**
* 问：小明有5个苹果，吃了2个，又买了3个，现在有几个？
* 答：**小明开始有5个苹果，吃了2个后剩下  个。接着他又买了3个，所以现在有  个。** 答案是6。



---

## 3. 论文的三个关键发现

### A. 涌现能力（Emergent Ability）

这是论文最震撼的结论之一：**思维链并不是在所有模型上都有效。**

* 当模型参数量较小（如小于 10B）时，CoT 反而可能降低准确率，因为模型没有能力写出逻辑。
* 只有当模型达到一定规模（通常指 100B 参数以上，如 PaLM 或 GPT-3）时，CoT 的效果才会爆发式增长。

### B. 显著的性能提升

在数学应用题数据集 **GSM8K** 上，使用 CoT 的 PaLM 540B 模型的表现大幅超过了当时最先进的微调模型，甚至让模型在某些任务上具备了接近人类水平的逻辑推导。

### C. 极强的鲁棒性

论文证明了 CoT 并不依赖于特定的提示词。无论是谁写的推理步骤，只要逻辑通顺，模型都能模仿这种思维模式并显著提高预测准确率。

---

## 4. 为什么 CoT 有效？

论文分析了几个深层原因：

1. **问题分解：** 它将复杂的多步问题分解为一系列可控的中间步骤。
2. **增加计算量：** 生成中间推理步骤实际上是让模型在给出最终答案前，消耗了更多的“思考时间”（Token 生成过程即计算过程）。
3. **可解释性：** 我们可以通过阅读模型的“思维链”来定位它在哪里算错了，这比黑盒式的直接输出要好得多。

---

## 5. 局限性

* **逻辑依然会幻觉：** 即使使用了 CoT，模型也可能在推理链条中出现逻辑断层或计算错误。
* **成本更高：** 因为要生成一大串中间步骤，消耗的 Token 更多，响应速度变慢。

---

### 总结

这篇论文告诉我们：**LLM 不仅仅是一个概率预测器，更是一个可以通过引导展现出逻辑能力的“推理引擎”。** 它开启了后续无数的研究方向，比如 *Self-Consistency*（多次投票）、*Tree of Thoughts*（思维树）等。
