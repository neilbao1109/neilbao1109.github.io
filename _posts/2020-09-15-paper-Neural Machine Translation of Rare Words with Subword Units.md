---
title: Neural Machine Translation of Rare Words with Subword Units
date: 2020-09-15 11:00:00 +0800
categories: [Paper, Machine Translation]
tags: [nmt, bpe, tokenization]
render_with_liquid: false
math: true
---

这篇论文由 Rico Sennrich 等人在 2016 年发表，是自然语言处理（NLP）领域具有**分水岭意义**的著作。它正式引入了 **BPE（Byte Pair Encoding，字节对编码）** 算法，解决了机器翻译中极其棘手的“未登录词”（Out-of-Vocabulary, OOV）和稀有词翻译问题。


---

### 1. 核心挑战：有限词表与稀有词

在 NMT（神经机器翻译）发展的早期，模型通常只能处理固定大小的词表（例如 30,000 到 50,000 个词）。

* **OOV 问题**：如果文本中出现了词表之外的词（如人名、地名、技术术语），模型通常只能将其标记为 `<UNK>`（未知词）。
* **形态丰富的语言**：像德语这种喜欢造“长难合成词”的语言，或者土耳其语这种通过词缀改变意义的语言，词汇量几乎是无限的，传统的词级模型根本无法覆盖。

### 2. 核心创新：字节对编码 (BPE)

作者提出：**不应该以“词”为基本单位，而应该以“子词”（Subword Units）为单位。** BPE 算法最初是一种数据压缩算法，作者将其引入 NLP：

1. **初始化**：将所有词拆分为字符序列（例如 `high` 变为 `h i g h`）。
2. **统计**：统计语料库中相邻单位对出现的频率。
3. **合并**：将出现频率最高的一对单位合并为一个新的单位（例如 `h i` 合并为 `hi`）。
4. **循环**：重复上述步骤，直到达到预设的子词词表大小。

---

### 3. BPE 的精妙之处

* **开表词表（Open Vocabulary）**：理论上，只要词表里包含了所有的基础字母，BPE 就能通过组合子词来表示任何单词。这意味着再也不会有 `<UNK>`。
* **捕捉词根与词缀**：BPE 能够自动学到有意义的语言结构。例如，它可能会把 `refactoring` 切分为 `re` + `factor` + `ing`。即使模型没见过 `refactoring`，只要见过其他带 `re-` 前缀或 `-ing` 后缀的词，也能推测出它的含义。
* **平衡效率与粒度**：它介于“词级”（词表太大）和“字符级”（序列太长、语义太薄弱）之间，找到了一个完美的平衡点。

### 4. 实验结论

* **性能提升**：在 WMT 15 的英德和英俄翻译任务中，BPE 显著超越了传统的反向翻译和词级模型。
* **解决音译问题**：对于人名和地名，BPE 可以通过拼写近似的方式实现跨语言的准确“翻译”（本质上是拼写转换）。

---

### 5. 论文的深远影响：现代 NLP 的基石

这篇论文的影响力远超机器翻译本身。如今，**子词分词（Subword Tokenization）** 已成为所有大语言模型（LLM）的标准配置：

* **GPT 系列**：使用的是 BPE。
* **BERT / Transformer**：使用的是 WordPiece（BPE 的变体）。
* **Llama / Mistral**：使用的是 SentencePiece（基于 BPE 的实现）。

可以说，如果没有这篇论文提出的子词方案，我们今天看到的 ChatGPT 依然会卡在如何理解“新词”或“罕见词”的问题上。
